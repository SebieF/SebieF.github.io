---
title: "AIMED-RL"
date: 2021-11-28
categories:
  - IT-Security
  - Tutorial
---

Automated, AI guided attacks have become a huge concern and hence research topic in recent years. The idea is indeed
frightening: Imagine an AI system which is able to bypass all the layers of security humans have created to secure
their systems. It will be able to take control over them and then...
Of course, we are still far away from such a scenario to take place. AI supported automatic malware modifications, however,
are already existing. These modifications can enable existing malicious software to evade detection by malware classifiers.
In order to be able to keep up with the latest possibilities that progress in AI research has
created for criminals to refine their attacks, I am advocating the idea that these ways of attacks have to be
published in an open source kind of format to be able to defense against them.
That is why **[AIMED-RL](https://github.com/SebieF/AIMED)**, **A**utomatic **I**ntelligent **M**alware modifications 
to **E**vade **D**etection using **R**einforcement **L**earning has been created. It was the topic of my bachelor's thesis
and is part of the bigger [AXMED](https://github.com/zRapha/AIMED) framework created and supervised by 
[Raphael Labaca-Castro](https://www.unibw.de/technische-informatik/mitarbeiter/wissenschaftliche-mitarbeiter-innen/m-sc-raphael-labaca-castro).
It focuses on portable executable (PE) **Windows binary files** 
(which can be manipulated e.g. by the [LIEF library](https://github.com/lief-project/LIEF)) and on evading **static 
detection**.
In this post I will describe its purpose and design. And in the end I will also provide an exhaustive tutorial on
how-to use it.

## Idea and Research Question 
Its purpose is quite simple: Deploy a reinforcement learning (RL) environment for functional malware modifications that
are able to evade static detection classifiers. This enables researchers and anti-virus companies to created large datasets
of adversarial malware files, that are modified by models that use the latest techniques from RL research.
Ideally, these adversarial files can uncover weaknesses in the detection models and harden them against attacks.

## Design
At first, I want to discuss the general RL workflow and thereby shed some light on how AIMED-RL works in the malware
environment. It is broken down to its most basic elements, please refer to the paper for more details.

{% raw %}<img src="https://sebief.github.io/assets/images/aimedRL/aimedrl-state.png" alt="">{% endraw %}
At the beginning, the environment chooses a random malware file from the training set. Using lief library,
a feature vector is calculated that contains both, PE specific features (such as number of sections in the PE file)
and structure-agnostic features (such as byte-level histograms). This feature vector is presented to the agent which
will feeds its neural network with the data.

{% raw %}<img src="{{ site.url }}{{ site.baseurl }}/assets/images/aimedRL/aimedrl-action.png" alt="">{% endraw %}
The output from the agent's neural network consists of a probability distribution. The agent chooses the action with
the highest score and sends its choice to the environment. Here, the malware will get modified with the chosen
modification.

{% raw %}<img src="{{ site.url }}{{ site.baseurl }}/assets/images/aimedRL/aimedrl-reward.png" alt="">{% endraw %}
Finally, the modification is evaluated by presenting the new modified malware file to the classifier (and applying
some other metrices). Thereby, the reward is calculated and sent to the agent which will update its neural network
accordingly. After that, the flow starts again if the malware file is not yet evading the detection classifier.

The environment is based on the [gym malware environment from Anderson et al](https://github.com/endgameinc/gym-malware).
It uses the OpenAI gym framework and implements the functionalities for the workflow to run.
An important aspect of the environment was the design of the **reward** function. In order to increase the diversity
of perturbations used by the agent, we introduced a **reward penalty** (TODO: refactor in AIMED-RL) with
decreases the agent's reward if it injects the same perturbation multiple times. This makes also sense on a technical
level because most perturbations can only change the PE file once. You can, however, easily switch the reward penalty
off and on by setting the reward_penalty flag in the PARAM dictionary.

For the agent, a lot of options are available to choose in the *rl.py* file. For the paper, we used a 
[combination](https://arxiv.org/abs/1710.02298) of a Distributional Double Deep Q-Network with Adam optimizer 
and Noisy Nets exploration. But of course, any algorithm compatible with 
[chainerrl](https://github.com/chainer/chainerrl) will work! 

## Tutorial - How to use AIMED-RL

### Setup
0. Create a new virtual (conda) environment and install the requirements from requirements.txt
1. Make sure *upx* is installed on your system (tested: 3.91, but more recent versions should be fine, too)

2. Please ensure that the main function in axmed.py looks like this:
```python
if __name__ == '__main__':
    scanner = 'GradientBoosting'
    main('AIMED-RL', scanner)
```

3. Configure the PARAM_DICT in the *rl.py* file to set the options you want AIMED-RL to work with.

### Train/Evaluate
1. Make sure to split your set of malware files in at least two separate sets: A training set (should be located in 
*samples/unzipped* for example, `PARAM_DICT["malware_path"]`) and an evaluation set (should be located in *samples/evaluate_set*).
*The evaluation data must never be included in the training data!*

2. Call axmed.py with --train and/or --eval to call the AIMED-RL workflow
```
# Train only:
python3 axmed.py --train 
# Train and eval:
python3 axmed.py --train --eval
# Only evaluate existing agent
python3 axmed.py -dir your-agent-directory --eval
```

### Interpret results
1. After a successful run of the AIMED-RL workflow, the *db* directory will look like this:
{% raw %}<img src="{{ site.url }}{{ site.baseurl }}/assets/images/aimedRL/aimedrl-eval.png" alt="">{% endraw %}

In the ..._data.csv files, you can find the data of every single turn that the agent has made. In contrast,
the ..._report.csv files provide general information about the training or evaluation results and the parameters
that have been used for training.

2.For more information about AIMED-RL please refer to the [paper](https://link.springer.com/chapter/10.1007/978-3-030-86514-6_3).
Thanks for your interest in AIMED-RL and AI based security research!
