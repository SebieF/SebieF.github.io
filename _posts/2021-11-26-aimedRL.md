---
title: "AIMED-RL"
date: 2021-09-07
categories:
  - IT-Security
  - Tutorial
---

Automated, AI guided attacks have become a huge concern and hence research topic in recent years. The idea is indeed
frightening: Imagine an AI system which is able to bypass all the layers of security humans have created to secure
their systems. It will be able to take control over them and then...
Of course, we are still far away from such a scenario to take place. AI supported automatic malware modifications, however,
are already existing. In order to be able to keep up with the latest possibilities that progress in AI research has
created for adversarials to refine their attacks, I am advocating the idea that these ways of attacks have to be
published in an open source kind of format to be able to defense against them.
That is why **AIMED-RL**, **A**utomatic **I**ntelligent **M**alware modifications 
to **E**vade **D**etection using **R**einforcement **L**earning has been created. 
In this post I will describe its purpose and design. And in the end I will also provide an exhaustive tutorial on
how-to use it.

## Idea and Research Question 
Its purpose is quite simple: Deploy a reinforcement learning (RL) environment for functional malware modifications that
are able to evade detection classifiers. This enables researchers and anti-virus companies to created large datasets
of adversarial malware files, that are modified by models that use the latest techniques from RL research.
Ideally, these adversarial files can uncover weaknesses in the detection models and harden them against attacks.

## Design
(SHOW PICTURES FROM RL FLOW - DESCRIBE ONE TURN)

The environment is based on the [gym malware environment from Anderson et al](https://github.com/endgameinc/gym-malware).
It uses the OpenAI gym framework and implements the functionalities for the workflow to run.
An important aspect of the environment was the design of the **reward** function. In order to increase the diversity
of perturbations used by the agent, we introduced a **reward penalty** (TODO: refactor in AIMED-RL) with
decreases the agent's reward if it injects the same perturbation multiple times. This makes also sense on a technical
level because most perturbations can only change the PE file once. You can, however, easily switch the reward penalty
off and on by setting the reward_penalty flag in the PARAM dictionary.

For the agent, a lot of options are available to choose in the rl.py file. For the paper, we used a 
[combination](https://arxiv.org/abs/1710.02298) of a Distributional Double Deeq Q-Network with Adam optimizer 
and Noisy Nets exploration. But of course, any algorithm compatible with 
[chainerrl](https://github.com/chainer/chainerrl) will work! 

## Tutorial - How to use AIMED-RL


0. Create a new virtual (conda) environment and install the requirements from requirements.txt
1. Make sure upx is installed on your system (tested: 3.91, but more recent versions should be fine, too)

2. Please ensure that the main function in axmed.py looks like this:
```python
if __name__ == '__main__':
    scanner = 'GradientBoosting'
    main('AIMED-RL', scanner)
```

3. Call axmed.py with --train and/or --eval to call the AIMED-RL workflow
```
# Train only:
python3 axmed.py --train 
# Train and eval:
python3 axmed.py --train --eval
# Only evaluate existing agent
python3 axmed.py -dir your-agent-directory --eval
```

4. For more information about AIMED-RL please refer to the [paper](https://link.springer.com/chapter/10.1007/978-3-030-86514-6_3),
to my blog [article]() or do not hesitate to create an issue!


